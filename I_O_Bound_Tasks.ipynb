{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVjr7tZuds34rGn0pQfWsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehjabeenalam-ux/Learning_Python-for-Beginners/blob/main/I_O_Bound_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I/O-Bound Tasks\n",
        "\n",
        "Spend most time waiting for external resources (network, disk, database, APIs, timers).\n",
        "CPU is mostly idle during waits.\n",
        "Best tools: asyncio (preferred modern choice), or threading."
      ],
      "metadata": {
        "id": "Y2YbP45BGOSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads the same webpage 20 times in a row, one after another.\n",
        "It measures and prints the size of each downloaded page in bytes, then shows the total time taken.\n",
        "\n",
        "It runs completely synchronously ‚Üí each request must fully finish before the next one starts, making it slow for many requests."
      ],
      "metadata": {
        "id": "nF6Z6uBPIfWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sequential (slow baseline)"
      ],
      "metadata": {
        "id": "5wgYeMQBGWTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyBEHnEQGLh4",
        "outputId": "742f9e16-0c78-44c3-f5c8-9c31240f9d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "513\n",
            "Total: 1.24 s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "urls = [\"https://example.com\"] * 20\n",
        "\n",
        "start = time.time()\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    print(len(response.content))\n",
        "print(f\"Total: {time.time() - start:.2f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version is the easiest to read and understand, which is why beginners (and many production scripts that only need 3‚Äì10 requests) still use it every day.\n",
        "\n",
        "If the number of requests grows to dozens or hundreds ‚Üí almost everyone switches to the asyncio version which is given below."
      ],
      "metadata": {
        "id": "HSYM9TYcH_Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Using asyncio (recommended for I/O-bound)"
      ],
      "metadata": {
        "id": "_ztZEy-sGcRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import time\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def fetch(url, session):\n",
        "    async with session.get(url) as response:\n",
        "        return await response.read()\n",
        "\n",
        "async def main():\n",
        "    urls = [\"https://example.com\"] * 50\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch(url, session) for url in urls]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        print([len(r) for r in results])\n",
        "\n",
        "start = time.time()\n",
        "asyncio.run(main())\n",
        "print(f\"Total: {time.time() - start:.2f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYcZemu2GfC9",
        "outputId": "3e81be65-6dc1-418a-d51d-ab451ca90b2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513, 513]\n",
            "Total: 0.37 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It asks the same website (‚Äúhttps://example.com‚Äù) 20 times in a row, downloads the page content each time, prints how many bytes it received, and finally shows how long the whole process took."
      ],
      "metadata": {
        "id": "Cn8ReV95GgwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads the same webpage 50 times, but in a very smart and fast way.\n",
        "\n",
        "Here‚Äôs what happens in simple words:\n",
        "\n",
        "- It makes a list of 50 copies of the same website address.\n",
        "- It asks the computer to start getting all 50 pages at almost the same time (not one after another).\n",
        "- While waiting for the internet to send each page, the computer works on the other requests.\n",
        "- When all pages arrive, it shows how big each one is.\n",
        "The whole thing finishes very quickly ‚Äî often in just a few seconds.\n",
        "\n",
        "- Normal slow way = one page at a time ‚Üí takes a long time.\n",
        "\n",
        "- This smart way = many pages together ‚Üí finishes much faster. üòä"
      ],
      "metadata": {
        "id": "LMIqJyH-JXxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using threading (still very good)"
      ],
      "metadata": {
        "id": "7-GVu8MqJvtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def fetch(url):\n",
        "    response = requests.get(url)\n",
        "    return len(response.content)\n",
        "\n",
        "urls = [\"https://example.com\"] * 50\n",
        "\n",
        "start = time.time()\n",
        "with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "    results = list(executor.map(fetch, urls))\n",
        "print(f\"Total: {time.time() - start:.2f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWOfFbRbJwK6",
        "outputId": "475d9cf0-f7d4-475e-984b-a6154d3d29b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 0.39 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads the same webpage 50 times in parallel using multiple threads.\n",
        "\n",
        "It creates a pool of 20 worker threads (using ThreadPoolExecutor)\n",
        "It sends up to 20 download requests at the same time\n",
        "Each thread runs the fetch() function ‚Üí makes one HTTP request with requests.get()\n",
        "executor.map() automatically gives each URL to a free thread\n",
        "When all 50 requests finish, it shows the total time taken\n",
        "\n",
        "Result: Much faster than doing one request after another\n",
        "(usually 3‚Äì8 seconds instead of 20‚Äì60 seconds), because waiting time for the internet overlaps between the threads."
      ],
      "metadata": {
        "id": "qYbK33xjKK24"
      }
    }
  ]
}